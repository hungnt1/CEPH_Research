

## 1. HIGH-LEVEL OPERATIONS


### 1.1. Operating Cluster 


- Xem status một MON
```
systemctl status ceph-osd@id_mon
```

- Stop các Ceph daemon
```
sudo systemctl stop ceph-mon\*.service ceph-mon.target
sudo systemctl stop ceph-osd\*.service ceph-osd.target
sudo systemctl stop ceph-mds\*.service ceph-mds.target


```


- Bật tất cả service
```
systemctl start ceph.target

```

- Bật service theo type daemon
```
systemctl start ceph-osd.target
systemctl start ceph-mon.target
systemctl start ceph-mds.target
```


- Xem các Ceph Service đang chạy trên node
```
systemctl status ceph\*.service ceph\*.target

```


### 1.2. Cluster Health Check 

- Kiêm tra LOG cho OSD
```
/var/log/ceph/ceph-osd.*

```


- Xem dung lượng của OSD và Pool 
```
[root@ceph_client ~]# ceph df

GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED 
    100 GiB     98 GiB      2.0 GiB          2.01 
POOLS:
    NAME         ID     USED     %USED     MAX AVAIL     OBJECTS 
    rbd_pool     1       0 B         0        31 GiB           0 

```


- Xem trạng thái của các Pool
```
cpeh osd pool stats
```

- Kiểm tra các thông số cụ thể trên các pool
```
ceph osd dump 
```

- Xem quota của các Pool
```
[root@ceph_client ~]# ceph df detail
GLOBAL:
    SIZE        AVAIL      RAW USED     %RAW USED     OBJECTS 
    100 GiB     98 GiB      2.0 GiB          2.01          0  
POOLS:
    NAME         ID     QUOTA OBJECTS     QUOTA BYTES     USED     %USED     MAX AVAIL     OBJECTS     DIRTY     READ     WRITE     RAW USED 
    rbd_pool     1      N/A               N/A              0 B         0        31 GiB           0        0       0 B       0 B          0 B 
```


- Cấu hình quota
```
ceph osd pool set-quota <poolname> max_objects <num-objects>
ceph osd pool set-quota <poolname> max_bytes <num-bytes>

ceph osd pool set-quota rbd_pool max_bytes 1024

```

- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


### 1.3.Monitoring  Cluster


- Hiển thị trạng thái Cluster
```
ceph -s / ceph status

ceph health

ceph health detail ## only warning and error status


```


- Xem log của cả cụm, dữ liệu được xuất từ /etc/ceph/ceph.loh
```
ceph -wc
```

- Kiểm tra dữ liệu được replicate và mở rộng trên các pool
```
ceph df 
```

- Kiểm tra Ratio
```
ceph osd dump | grep full_ratio

```

- Hiểm thị các ODS dưới dạng tree
```
osd osd tree
```

- Kiểm tra trạng thái các MON
```
osd mon stat 

ceph mon dump

```

- Kiểm tra quorum status của cluster
```
ceph quorum_status
```

- Kiểm trang mạng thái MDS
```
ceph mds stat
```

- Kiểm tra trạng thái MDS cluster
```
ceph fs dump 
```


### 1.4. MONITORING OSDS AND PGS

- Ceph data placement giúp các dữ liệu được truyền vào cụm không bị rằng buộc một địa chỉ trực tiếp trên các OSD chungs được đặt trên.  Việc theo dõi dữ liệu trên cả cụm sẽ liên quan đến OSD và PGS là chủ yếu./ 


- Một OSD sẽ bao gồm 2 trạng thái trọng cụm in - the cluster và out - the cluster , bên cạnh đó sẽ có 2 trạng thái service up running và down - not running. Trong trường hợp OSD đi vào trạng thái out khỏi cụm, Ceph sẽ di dời placement groups sang OSD khác. 

- Xem trạng thái cơ bản của các OSD
```
ceph osd stat

```


- List OSD ỏ trạng thái up
```
ceph osd status
```

- Dump OSD Content
```
ceph osd dump

```

- List tất cả các OSD đang có trong CRUSH MAP
```
ceph osd tree
```

- Khởi động OSD ở trạng thái down
```
systemctl start ceph-osd@1
```

- Khi CRUSH thực hiện chỉ định  placement group cho các OSD, nó sẽ xem xét số repicate của pool, và gán các placement cho các OSD sao cho mỗi bảo sao của placement group được gán trên các OSD khác nhau. Ví dụ nếu pool yêu cầu 3 bản replica cho các placment group, CRUSH sẽ chỉ định chúng vào osd.1, osd.2 and osd3. 


- Xem trạng thái cơ bản của các pg
```
ceph pg stat

```

- Liệt kê các placement group
```
ceph pg dump

```

- Trạng thái :
    - ACTIVE : Once Ceph completes the peering process, a placement group may become active. The active state means that the data in the placement group is generally available in the primary placement group and the replicas for read and write operations.
    - CLEAN : When a placement group is in the clean state, the primary OSD and the replica OSDs have successfully peered and there are no stray replicas for the placement group. Ceph replicated all objects in the placement group the correct number of times.

- http://docs.ceph.com/docs/mimic/rados/operations/monitoring-osd-pg/


- Xem vị trí của một object
```
ceph osd map {poolname} {object-name} [namespace]

rados -p data ls

```

### 1.5 : USER MANAGEMENT

- Khi Ceph chạy ở mode yêu cầu xác thực, nếu không chỉ định user. `client.admin ` sẽ được chỉ định là user mặc định. và sẽ sử dụng `keyring` trong /etc/ceph nếu không chỉ định keyring.

```
ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health

```

- Pool được chỉ ddinhj trong Ceph để thực hiện viết, đọc dữ liệu không bất bể Ceph Client nào. Các user được ủy nhiệm đặc quyền sẽ được ceph cho phép làm việc với các pool này. và dữ liệu trong nó. Với ceph khái niệm người dùng được quản lý được xếp vào "client". Form user trong CEPH : TYPE.ID


- Ceph sử dụng "capabilities" để thể hiện các quyền được mà các user có thể làm việc với các MON, OSD, metadata server. 
```
{daemon-type} '{cap-spec}[, {cap-spec} ...]'

```

- http://docs.ceph.com/docs/mimic/rados/operations/user-management/#background


- Xem user trên cluster
```
ceph auth ls

```

- Xóa một user
```
ceph auth del  {TYPE}.{ID}
```

- Khởi tạo kho chứa key
```
ceph-authtool -C /etc/ceph/ceph.keyring
```


- Khởi tạo 1 user
```
ceph-authtool -C /etc/ceph/ceph.client.nguyenhungsync.keyring -n client.nguyenhungsync --cap osd 'allow *'  --cap mon 'allow *' --cap mds 'allow *' --cap mgr 'allow *' --gen-key
ceph auth add client.nguyenhungsync -i /etc/ceph/ceph.client.nguyenhungsync.keyring
```


- Xem các cap của user

```
ceph auth get client.admin

```


## 2 . DATA PLACEMENT OVERVIEW

- Ceph lưu trữ và replicate các data object trên một  cụm  một cách linh động và có kế hoạch : 
    - Pools : ceph lưu trữ dữ liệu trong các pool, chứa các logical group. Pool quản lý các placment group , số lượng replicate, và CRUSH Rule cho pool . Để lưu trữ dữ liệu vào pool cần thao tác truyên người dùng được ủy quyền.
    - Placement Groups: CEPH sẽ không mapping location cụ thể các object trên các ODS, thay vi đó sẽ mapping vào placement group ( PG ). Placement sẽ đặt các object các object cùng 1 nhóm . 
    - CRUSH Maps : sẽ giảm hiện tượng nghẽn cổ chai dựa vào thuật toán cruss, cung cấp khả năng xác định nơi lưu dữ liệu ( PD group), cách sao lưu dữ liệu và nơi các bản ghi  sao chép sẽ được lưu trữ


- Việc khởi tạo 1 pool mới tương đương với việc khởi tạo một I/O interface cho các client để lưu trữ dữ liệu .Ceph client  làm việc với Cluster rất đơn giản : bắt tay và kết nối với cụm, khởi tạo I/O tuy thuộc vào reading và wrinting để lưu trữ object và các thuộct tính bổ sung . Để kết nối tới Cluster storage, cacs client cần yêu cầu có ít nhất 3 trường : "name_cluster"  + "mon_address" và secret key - user cho việc xác thực. 
![](images/19.png)



- Khi khởi tạo một cluster mới, Ceph sẽ sử dụng default pool  để lưu trữ dữ liệu . Một pool cung có thể cung cấp các khả năng sau đây :
    - Resilience : chỉ định số OSD có thể ở status fail ở mode replicas . Ví dụ với   replicated pools sẽ xác định số lượng số OSD = 3 , tương ứng với số bản copy = 3, nêu số ODS ở status fail nhỏ hơn 2, pool sẽ ngừng hoạt động. 
    - Placement Groups : chỉ số lượng placement groups  cho pool 
    - CRUSH Rules: khi đặt dữ liệu vào một rule , vị trí của dữ liệu và bản sao trong cụm được điều chỉnh dựa vào các rule
    - Snapshots : khả năng snapshot nhanh 1 pool.
    - Quotas: cung cấp khả năng giới hạn số byte cũng như số object có thể lưu trữ trên pool .

    

- Xem các pool đang có
```
[root@ceph_node1 ~]# ceph osd lspools
1 rbd_pool
2 pool_block
```

- Cấu hình số PG và PGD mặc định
```
osd pool default pg num = 100
osd pool default pgp num = 100
```

- Khởi tạo pool mới
```
ceph osd pool create {pool-name} {pg-num} [{pgp-num}] [replicated] \
     [crush-rule-name] [expected-num-objects]
ceph osd pool create {pool-name} {pg-num}  {pgp-num}   erasure \
     [erasure-code-profile] [crush-rule-name] [expected_num_objects]

ceph osd pool create pool_block 100 100  replicated

```
- Trong đó : 
    - {pool-name} : tên của pool
    - {pg-num} :  tổng số placment group của group ( bao gồm số placement group được sử dụng và không sử dụng)
    - {pgp-num} : tổng số placement groups được sử dụng  (thông thường bằng tổng số pg-num )
    - {replicated|erasure} : replicated - cho phép các pool phục hồi object bằng cách lưu trữ nhiều bản sao cảu Object. erasure cho phép thực hiện cơ chế  RAID5 cho các object. [](https://vi.wikipedia.org/wiki/RAID#/media/File:RAID_5.svg)
    - crush-rule-name : crush rule được sử dụng cho pool
    - [expected-num-objects]: số object trên pool





- Gắn Pool vào một application. Pool cần phải được liên kết với một ứng dụng trước khi sử dụng. Ví dụ với pool được gắn vào RBD sẽ được khởi tạo bằng rbd tool. Cho phép pool cấm vận các client không được acces vào pool type. CephFS uses the application name "cephfs", RBD uses the application name "rbd", and RGW uses the application name "rgw".
```
ceph osd pool application enable {pool-name} {application-name}


ceph osd pool application enable pool_block rbd

```


- Pool quota
```
ceph osd pool set-quota {pool-name} [max_objects {obj-count}] [max_bytes {bytes}]


ceph osd pool set-quota pool_block max_objects 100 ## set

ceph osd pool set-quota pool_block max_objects 0  ## unset 
```

- Để xoá một pool, cần cấu hình trên MON cho phép xoá pool
```
ceph tell mon.* injectargs --mon-allow-pool-delete=true ## allow 

ceph tell mon.* injectargs --mon-allow-pool-delete=false ## denied


```

- Thực hiện xoá Pool
```
ceph osd pool delete {pool-name} [{pool-name} --yes-i-really-really-mean-it]



[]# ceph osd pool delete pool_block pool_block --yes-i-really-really-mean-it
pool 'pool_block' removed


```


- Đổi tên Pool
```
ceph osd pool rename {current-pool-name} {new-pool-name}

ceph osd pool rename {current-pool-name} {new-pool-name}

[]# ceph osd pool rename rbd_pool rbd_pool_1
pool 'rbd_pool' renamed to 'rbd_pool_1'

```

- Xem thông kê Pool
```
rados df
``` 

- Tạo snapshot cho pool
```
ceph osd pool mksnap {pool-name} {snap-name}


[]# ceph osd pool mksnap rbd_pool_1 snap_1
created pool rbd_pool_1 snap_1

```

- Xoá một snap trên pool
```
ceph osd pool rmsnap {pool-name} {snap-name}


[]# ceph osd pool rmsnap rbd_pool_1 snap_1
```

- Set Pool Value 
```
ceph osd pool set {pool-name} {key} {value}


ceph osd pool set rbd_pool_1 size 3
ceph osd pool set rbd_pool_1 min_size 2


```

- Một số key value . 

|Key|Value|
|---|---|
| compression_algorithm| `lz4`, `snappy`, `zlib`, `zstd`|
|compression_mode|`none`,  `passive`,  `aggressive`,  `force`|
|compression_min_blob_size| số byte của object tối thiểu cần thực hiện nén|
|compression_max_blob_size|byte ( interger )|
|size|số replica|
|min_size| số replicat tối thiểu|
|pg_num| số placment group|
|pgp_num| số placment group được sử dụng |
|crush_rule| crush rule áp dụng cho các placment|
|Xem thêm |http://docs.ceph.com/docs/mimic/rados/operations/pools/#set-pool-values|

- Xem value của Pool
```
ceph osd pool get {pool-name} {key}

[]# ceph osd pool get rbd_pool_1 size

```

- Xem các cấu hình của Pool
```
ceph osd dump | grep "rbd_pool_1"

```



## 3.  Erasure Code Pools


- Tuỳ thuộc vào yêu cầu đồ bền của dữ liệu việc xác định cách lưu trữ dữ liệu có thể được lựa chọn. Đồ bền của dữ liệu có nghĩa là khả năng đáp ứng dữ liệu mà không có mất mát trong trường hợp mất hoặc nhiều hơn OSD.
- Ceph lưu trữ dữ liệu trên các pool và sẽ có 2 kiểu pool có thể lựa chọn :
    - replicated
    - erasure-coded

- Ceph sử dụng pool replicated làm mặc định cho các object, có nghĩa rằng các bản copy của dữ liệu có thể trải đề trên các OSD. 


- erasure-coded pool đáp ứng nhu cầu bên vững nhu cầu bền vững dữ liệu hơn replicated với thụât toán riêng, sử dụng nhiều diskspace đồng thời tăng khả năng bền vững. 


- https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/3/html-single/storage_strategies_guide/index#erasure_code_poolsfa

## 5. PLACEMENT GROUPS

- Tính số PG dựa vào số OSD
    - Less than 5 OSDs set pg_num to 128
    - Between 5 and 10 OSDs set pg_num to 512
    - Between 10 and 50 OSDs set pg_num to 1024
    - If you have more than 50 OSDs, you need to understand the tradeoffs and how to calculate the pg_num value by yourself
    - For calculating pg_num value by yourself please take help of pgcalc tool



### 4.1 : HOW ARE PLACEMENT GROUPS USED ?

- Một hệ thống lớn với số lượng lớn các object sẽ không thể theo dõi từng object thay vì đó, Ceph chia nhỏ các pool bằng các placement group , gắn từng object riêng lẻ cho tầng các placement group , các placement group sẽ được gắn vào OSD chính . Nếu một OSD đi vào trạng thái fail. cluster sẽ tự động cân bằng lại bằng các thực hiện replicate các placement group, với pool size bằng 2 thì placment sẽ cố gắng copy các oject trong các placement group 2 lần .

![](images/22.png)
- Khi CRUSH thực hiện gán placemtn group  tới một OSD, nó sẽ tìm kiếm các OSD phù hợp và xác định OSD primary. Primary OSD thực hiện sử dụng CRUSH để tìm kiếm các secondary OSD  sau đó thực hiện copy  placement group’s contents . 


![](images/23.png)
- Khi primary thi vào trạng thái fail, CRUSH sẽ thực hiện chuyeẻn đổi primary OSD, thực hiện quyền nắm giữ các placement group và thực hiện sao chép các object sang OSD mới. 


### 4.1 : Data Durability

- Pending

### 4.2 :  Data Distribution within a pool

- Các object trong pool được phân bố đều trên các placement group. Khi CRUSH thực hiện  tính toán placement group cho mỗi object, CRUSH sẽ không biết được thật sự có bao nhiêu data đã được lưu trữ sẵn dưới các OSD, việc này được quản lý bởi "ratio", tỉ lệ giữa số placement group và cố OSD sẽ ảnh hưởng đến việc phân phối dữ liệu

- Ví dụ : trên một pool có replica size 3 chứa một placement group có 10 OSD, trong trường hợp này CRUSH sẽ chỉ sử dụng đến 3 OSD. Khi số placement group được thêm, các object sẽ được trải trều trên các OSD. CRUSH sẽ thực hiện chia để các data trên các OSD của các placement group . 


- Pending


### 4.3 : CHOOSING THE NUMBER OF PLACEMENT GROUPS

- Pending

